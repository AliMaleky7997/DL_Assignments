{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drDKyoDznPMP"
   },
   "source": [
    "# CE-40959: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwNpC5conPMS"
   },
   "source": [
    "# HW3. Part 2. NumPy Based CNN block (20 + 10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dusX00hNnPMV"
   },
   "source": [
    "### Deadline:   16 Farvardin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nud1FqU0nPMX"
   },
   "source": [
    "##   Outline of the Assignment\n",
    "\n",
    "You will be implementing the building blocks of a convolutional neural network\n",
    "\n",
    "1. **`zero padding`**\n",
    "\n",
    "2. **`convolution : Forward`**\n",
    "\n",
    "3. **`convolution : Backwrd`**\n",
    "\n",
    "4. **`Max pooling`**\n",
    "\n",
    "5. **`Batch Normalization in CNN `**\n",
    "\n",
    "\n",
    "    \n",
    "This notebook will ask you to implement these functions from scratch in **`Numpy`**.\n",
    "\n",
    "\n",
    "**Note** that for every forward function, there is its corresponding backward equivalent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlGba2SdnPMZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVLBNfwjnPMh"
   },
   "source": [
    "## 1. Zero Padding (2 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIofak2KnPMk"
   },
   "source": [
    "Zero-padding adds zeros around the border of an image:\n",
    "\n",
    "**Exercise**  : Implement the following function, which pads all the images of a batch of examples X with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Xac07WonPMn"
   },
   "source": [
    "shape of X and its zero pad array is :\n",
    "\n",
    "\n",
    "$$ X : (N, C, i_h, i_w)   $$\n",
    "$$  \\text{zeropad}(X) : (N, C, i_h + 2*ph, i_w + 2*pw)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYa5kwC7nPMp"
   },
   "source": [
    "**Note** : you should not use np.pad in your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2rbQl4enPMr"
   },
   "outputs": [],
   "source": [
    "def zero_padding(X, padding):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image.\n",
    "\n",
    "    input :\n",
    "      - X :  numpy array of shape (N, C, IH, IW) representing a batch of N images\n",
    "      - padding : a tuple of 2 integer (ph, pw), amount of padding around each image on vertical and horizontal dimensions\n",
    "    return :\n",
    "      - zero_pad : zero pad array of shape (N, C, IH + 2*ph, IW + 2*pw)\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    zero_pad = None\n",
    "    ###########################################################################\n",
    "    # Hint: you should not use the function np.pad for padding.                     \n",
    "    ###########################################################################\n",
    "    \n",
    "    n, c, ih, iw = X.shape\n",
    "    ph, pw = padding\n",
    "    zero_pad = np.zeros((n,c,ih + 2*ph, iw + 2*pw))\n",
    "    zero_pad[:,:,ph:ih+ph,pw:iw+pw] = X\n",
    "#     for i in range(n):\n",
    "#         for j in range(c):\n",
    "#             for k in range(ph, ih+ph):\n",
    "#                 for l in range(pw, iw+pw):\n",
    "#                     zero_pad[i,j,k,l] = X[i,j,k-ph,l-pw]\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return zero_pad\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOvCLShTnPMy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your implementation is correct\n",
      "shape of x is : (2, 3, 4, 4)\n",
      "shape of x_pad is : (2, 3, 10, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20bccde4ba8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test zero_padding function\n",
    "np.random.seed(1968)\n",
    "\n",
    "x = np.random.rand(2, 3 ,4, 4)\n",
    "padding = (3, 2)\n",
    "x_pad = zero_padding(x, padding)\n",
    "\n",
    "\n",
    "assert x_pad.shape==(x.shape[0], x.shape[1], x.shape[2] + 2*padding[0], x.shape[3] + 2*padding[1])\n",
    "assert np.all(x_pad[:, :, padding[0]:padding[0]+x.shape[2], padding[1]:padding[1]+x.shape[3]]==x)\n",
    "\n",
    "print(\"your implementation is correct\")\n",
    "print(\"shape of x is :\", x.shape)\n",
    "print(\"shape of x_pad is :\", x_pad.shape)\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, 0, :, :])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yaLgNcJonPM5"
   },
   "source": [
    "## 2.convolution : Forward (4 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSHkDYrfnPM7"
   },
   "source": [
    "In this Exercise, you implement convolutional neural networks using the NumPy library only.\n",
    "\n",
    "The input X,W are the input of the convolutional layer and the shape of X,W are $(N, C, i_h, i_w)$ , $(F, C, f_h, f_w)$ respectively and The return  value O is the output of the convolutional layer and the shape is $(N, F, O_h, O_w)$ where :\n",
    "\n",
    "$$\\text{stride} : (s_h,s_w)$$\n",
    "\n",
    "$$\\text{padding} : (p_h,p_w)$$\n",
    "\n",
    "$$O_w =\\lfloor \\frac{i_w - f_w + 2*p_w}{s_w} \\rfloor + 1$$\n",
    "\n",
    "$$O_h = \\lfloor\\frac{i_h - f_h + 2*p_h}{s_h}\\rfloor + 1$$\n",
    "$$O(b,f, i ,j)=\\sum_{r=0}^{C-1}\\sum_{k=0}^{f_h-1}\\sum_{l=0}^{f_w-1} W(f,r,k,l) X(b,r,s_h *i +k, s_w  *j +l)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc6Tt8EGnPM9"
   },
   "outputs": [],
   "source": [
    "def convolution2D(X, W, stride, padding):\n",
    "    \"\"\"\n",
    "    A implementation of the forward pass for a convolutional layer.\n",
    "    \n",
    "    The input consists of N data points, each with C channels, height IH and\n",
    "    width IW .We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height FH and width FW.\n",
    "    \n",
    "    \n",
    "    inputs:\n",
    "     - X : input data of shape (N, C, IH, IW)\n",
    "     - W : Filter weight of shape (F, C, FH, FW)\n",
    "     - stride : a tuple of 2 integer (sh, sw)\n",
    "     - padding :a tuple of 2 integer (ph, pw)\n",
    "     \n",
    "    return:\n",
    "     - out : Output data, of shape (N, F, OH, OW) where OH and OW given by\n",
    "     \n",
    "     OH= 1 + int ( (IH + 2*ph - FH)/ sh )\n",
    "     OW= 1 + int ( (IW + 2*pw - FW)/ sw )\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # Implement the convolutional forward pass.                               #\n",
    "    ###########################################################################\n",
    "    N, c, ih, iw = X.shape\n",
    "    f, _, fh, fw = W.shape\n",
    "    sh, sw = stride\n",
    "    ph, pw = padding\n",
    "    oh = 1 + int((ih+2*ph-fh)/sh)\n",
    "    ow = 1 + int((iw+2*pw-fw)/sw)\n",
    "    \n",
    "    out = np.zeros((N, f, oh, ow))\n",
    "    x = zero_padding(X, padding)\n",
    "    \n",
    "    for m in range(oh):\n",
    "        for n in range(ow):\n",
    "            out[:,:,m,n] = (W.reshape(1,f,c,fh,fw)*x[:,:,sh*m:sh*m+fh,sw*n:sw*n+fw].reshape(N,1,c,fh,fw)).sum(axis=(2,3,4))\n",
    "    \n",
    "#     for i in range(n):\n",
    "#         for j in range(f):\n",
    "#             for m in range(oh):\n",
    "#                 for n in range(ow):\n",
    "#                     out[i,j,m,n] = np.sum(W[j,0:c,0:fh,0:fw]*x[i,0:c,sh*m:sh*m+fh,sw*n:sw*n+fw])\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuqhD5E8nPNJ"
   },
   "source": [
    "To test your implementation, we will compare the results  with tensorflow function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ran5YbunPNM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : 3.6476928257419586e-27\n",
      "output shape : (2, 7, 8, 4)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1973)\n",
    "param1 = {'X':np.random.rand(2, 3, 23, 20), 'W':np.random.rand(7, 3, 6, 6), 'stride':(3, 6), 'padding':(2, 3)}\n",
    "x_tf = np.pad(param1['X'], ((0, 0), (0, 0), (2, 2), (3, 3)), 'constant', constant_values=0)\n",
    "\n",
    "\n",
    "conv_numpy = convolution2D(**param1)\n",
    "conv = tf.nn.conv2d(tf.transpose(x_tf, [0 ,2, 3, 1]), tf.transpose(param1['W'], (2, 3, 1, 0)), [1, 3, 6, 1], 'VALID')\n",
    "conv = tf.transpose(conv, (0, 3, 1, 2))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    conv = sess.run(conv)\n",
    "\n",
    "\n",
    "assert conv.shape==conv_numpy.shape\n",
    "print(\"Error :\", (np.sum(conv - conv_numpy)**2))\n",
    "print(\"output shape :\", conv_numpy.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yam8Y2x8nPNe"
   },
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **out shape**\n",
    "        </td>\n",
    "        <td>\n",
    "            (2, 7, 8, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **ERROR**\n",
    "        </td>\n",
    "        <td>\n",
    "            2.5559093329160782e-28\n",
    "       </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRFXIZfwnPNg"
   },
   "source": [
    "## 3.convolution : Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEf-K0MKnPNi"
   },
   "source": [
    "### 3.1 - Backward  w.r.t. filter (4 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1guVkIsfnPNk"
   },
   "source": [
    "This is the formula for computing a $\\frac{\\partial L}{\\partial W}$ for a single $W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )$ that $W$ is 4-D array as a filter in convolution operation with shape $(F,C,f_h,f_w)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LmJRQzNRnPNm"
   },
   "source": [
    "$$\\frac{\\partial L}{\\partial W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )} = \\sum_{b=0}^{N-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b,f^\\prime,i,j)} \\frac{\\partial O(i,j)}{\\partial W(f^\\prime,c^\\prime,k^\\prime ,l^\\prime )}\\right ) = \\sum_{b=0}^{N-1}\\left (\\sum_{i=0}^{O_w-1}\\sum_{j=0}^{O_h-1} \\frac{\\partial L}{\\partial O(b,f^\\prime,i,j)}  X(b,c^\\prime, s_h*i +k^\\prime, s_w*j +l^\\prime) \\right )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2g-PgNmnPNo"
   },
   "outputs": [],
   "source": [
    "def convolution2D_backward_filter(out_grad, X, W, stride):\n",
    "    \"\"\"\n",
    "    A implementation of the backward pass for a convolutional layer.\n",
    "    \n",
    "    inputs:\n",
    "     - out_grad  : gradient of the Loss with respect to the output of the conv layer with shape (N, F, OW, OH)\n",
    "     - X : input data of shape (N, C, IH, IW)\n",
    "     - W : Filter weight of shape (F, C, FH, FW)\n",
    "     - stride : a list of [sh, sw]\n",
    "     \n",
    "    return:\n",
    "     - dW : Gradient with respect to W\n",
    "    \n",
    "    \"\"\"\n",
    "    dW = None\n",
    "    ###########################################################################\n",
    "    # Implement the convolutional backward pass.                              #\n",
    "    ###########################################################################\n",
    "    \n",
    "    n, f, ow, oh = out_grad.shape\n",
    "    n, c, ih, iw = X.shape\n",
    "    f, c, fh, fw = W.shape\n",
    "    sh, sw = stride\n",
    "    \n",
    "    dW = np.zeros_like(W)\n",
    "    for fp in range(f):\n",
    "        for cp in range(c):\n",
    "            for kp in range(fh):\n",
    "                for lp in range(fw):\n",
    "                    dW[fp,cp,kp,lp] = np.sum(out_grad[0:n,fp,0:ow,0:oh] * X[0:n, cp, kp:sh*ow+kp:sh, lp:sw*oh+lp:sw])\n",
    "#                     for b in range(n):\n",
    "#                         for i in range(ow):\n",
    "#                             for j in range(oh):\n",
    "#                                 dW[fp,cp,kp,lp] += out_grad[b,fp,i,j] * X[b,cp,sh*i+kp,sw*j+lp]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return dW\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5GrLdnlnPNu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  : 1.4546595092275458e-27\n",
      "dW_tf  : 5340.576411697173\n",
      "dW  : 5340.576411697173\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1345)\n",
    "\n",
    "param = {'X':np.random.rand(2, 3, 10,10), 'W':np.random.rand(7, 3, 4, 4), 'stride':(2, 2)}\n",
    "c_1 = np.ones((2, 7, 4, 4))   \n",
    "dw = convolution2D_backward_filter(c_1, **param)\n",
    "\n",
    "\n",
    "\n",
    "w = tf.Variable(tf.transpose(param['W'],(2, 3, 1, 0)),name='v')\n",
    "c = tf.nn.conv2d(tf.transpose(param['X'],[0, 2, 3, 1]), w, [1, 2, 2, 1], 'VALID')\n",
    "loss = tf.reduce_sum(c)\n",
    "dw_tf = tf.gradients(loss, w)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    dw_tf = sess.run(dw_tf)\n",
    "\n",
    "\n",
    "\n",
    "dw = np.transpose(dw, (2, 3 ,1, 0))\n",
    "print(\"Error  :\", np.sum((dw-dw_tf[0])**2))\n",
    "print(\"dW_tf  :\", np.sum(dw_tf[0]))\n",
    "print(\"dW  :\", np.sum(dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzXtSW_InPN0"
   },
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dW_tf**\n",
    "        </td>\n",
    "        <td>\n",
    "            5340.576411697173\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dW**\n",
    "        </td>\n",
    "        <td>\n",
    "            5340.576411697173\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Error**\n",
    "        </td>\n",
    "        <td>\n",
    "            2.473867798773093e-27\n",
    " </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxyz9o2GnPN3"
   },
   "source": [
    "### 3.2 - Backward  w.r.t. input (4 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7P5oyWXnPN5"
   },
   "source": [
    "This is the formula for computing a $\\frac{\\partial L}{\\partial X}$ for a single $X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )$ that $X$ is 4-D array as a input in convolution operation with shape $(N,C,i_h,i_w)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLGji2fKnPN7"
   },
   "source": [
    "$$\\frac{\\partial L}{\\partial X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )} = \\sum_{f=0}^{F-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b^\\prime,f,i,j)} \\frac{\\partial O(b^\\prime,f,i,j)}{\\partial X(b^\\prime,c^\\prime,k^\\prime ,l^\\prime )}\\right ) = \\sum_{f=0}^{F-1}\\left (\\sum_{i=0}^{O_h-1}\\sum_{j=0}^{O_w-1} \\frac{\\partial L}{\\partial O(b^\\prime,f,i,j)} W(f,c^\\prime,k^\\prime - s_h*i, l^\\prime - s_w*j) \\right )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9-Ez6OQnPN-"
   },
   "outputs": [],
   "source": [
    "def convolution2D_backward_input(out_grad, X, W, stride):\n",
    "    \"\"\"\n",
    "    A implementation of the backward pass for a convolutional layer.\n",
    "    \n",
    "    inputs:\n",
    "     - out_grad  : gradient of the Loss with respect to the output of the conv layer with shape (N, F, OW, OH)\n",
    "     - X : input data of shape (N, C, IH, IW)\n",
    "     - W : Filter weight of shape (F, C, FH, FW)\n",
    "     - stride : a list of [sh, sw]\n",
    "     \n",
    "    return:\n",
    "     - dX : Gradient with respect to X\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    dX = None\n",
    "    ###########################################################################\n",
    "    # Implement the convolutional backward pass.                              #\n",
    "    ###########################################################################\n",
    "    \n",
    "    n, f, ow, oh = out_grad.shape\n",
    "    n, c, ih, iw = X.shape\n",
    "    f, c, fh, fw = W.shape\n",
    "    sh, sw = stride\n",
    "    \n",
    "    dX = np.zeros_like(X)\n",
    "    for bp in range(n):\n",
    "        for cp in range(c):\n",
    "            for kp in range(ih):\n",
    "                for lp in range(iw):\n",
    "                    for ff in range(f):\n",
    "                        for i in range(oh):\n",
    "                            for j in range(ow):   \n",
    "                                if kp-sh*i>=0 and kp-sh*i<fh and lp-sw*j>=0 and lp-sw*j<fw:\n",
    "                                    dX[bp,cp,kp,lp] += out_grad[bp,ff,i,j] * W[ff,cp,kp-sh*i,lp-sw*j]\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return dX\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJwwMZqInPOE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error is : 0.0\n",
      "dX_tf is : 208.39287018595633\n",
      "dX is : 208.39287018595633\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1992)\n",
    "\n",
    "param = {'X':np.random.rand(5, 3, 6, 6), 'W':np.random.rand(2, 3, 2, 2), 'stride':(3,3)}\n",
    "grad = np.ones((5, 2, 2, 2))\n",
    "dx = convolution2D_backward_input(grad, **param)\n",
    "\n",
    "\n",
    "\n",
    "w = tf.Variable(tf.transpose(param['W'], (2, 3, 1, 0)), name='v')\n",
    "x = tf.Variable(tf.transpose(param['X'], [0, 2, 3, 1]), name='x')\n",
    "c = tf.nn.conv2d(x, w, [1,3,3,1], 'VALID')\n",
    "loss = tf.reduce_sum(c)\n",
    "g = tf.gradients(loss, x)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    dx_tf = sess.run(g)\n",
    "dx = np.transpose(dx, (0, 2, 3, 1))\n",
    "\n",
    "\n",
    "\n",
    "assert dx.shape==dx_tf[0].shape\n",
    "print(\"Error is :\", np.sum((dx-dx_tf[0])**2))\n",
    "print(\"dX_tf is :\", np.sum(dx_tf[0]))\n",
    "print(\"dX is :\", np.sum(dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmIIdIwqnPOL"
   },
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dX_tf**\n",
    "        </td>\n",
    "        <td>\n",
    "            208.39287018595633\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dX**\n",
    "        </td>\n",
    "        <td>\n",
    "            208.39287018595633\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Error**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.0\n",
    " </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01DiIHblnPOO"
   },
   "source": [
    "## 4.Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s_-ssYznPOQ"
   },
   "source": [
    "### 4.1 - forward max pooling (3 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zuq2YNg1nPOS"
   },
   "source": [
    "The pooling layer reduces the height and width of the input. It helps reduce computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_SFsAmLnPOV"
   },
   "source": [
    " - Max-pooling layer: slides an ($f, f$) window with stride $s$ over the input and stores the max value of the window in the output.\n",
    "\n",
    "in function below X is input and shape of X is $(N, C, i_h, i_w)$  and output is shape $(N, C, O_h, O_w)$ that :\n",
    "\n",
    " $$O_h =\\lfloor\\frac{i_h - f }{s}\\rfloor + 1$$\n",
    " $$O_w =\\lfloor\\frac{i_w - f }{s}\\rfloor + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEYR4RCNnPOX"
   },
   "source": [
    "**Exercise**: Implement the forward pass of the pooling layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaWcSaPKnPOZ"
   },
   "outputs": [],
   "source": [
    "def pool_forward(X, f, s):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    input:\n",
    "       - X : numpy array of shape (N, C, IH, IW)\n",
    "       - f : int, filter size in height and width dim\n",
    "       - s : int\n",
    "    \n",
    "    Returns:\n",
    "       - pool : output of the pool layer, a numpy array of shape (N, C, OH, OW) where OH and OW given by\n",
    "       \n",
    "       OH = 1 + int((IH - f)/s)\n",
    "       OW = 1 + int((IW - f)/s)\n",
    "    \n",
    "    \"\"\"\n",
    "    pool = None\n",
    "    ###########################################################################\n",
    "    #  Implement the max pooling forward pass.                                #\n",
    "    ###########################################################################\n",
    "    \n",
    "    n, c, ih, iw = X.shape\n",
    "    oh = 1 + int((ih - f)/s)\n",
    "    ow = 1 + int((iw - f)/s)\n",
    "    \n",
    "    pool = np.zeros((n,c,oh,ow))\n",
    "    for i in range(n):\n",
    "        for j in range(c):\n",
    "            for k in range(0,oh):\n",
    "                for l in range(0,ow):\n",
    "                    pool[i,j,k,l] = np.max(X[i,j,k*s:k*s+f,l*s:l*s+f])\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return pool\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtN8FqAlnPOj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your implementation is correct\n",
      "output shape : (2, 3, 2, 2)\n",
      "output : [[[[0.46800661 0.6818562 ]\n",
      "   [0.95191188 0.58676102]]\n",
      "\n",
      "  [[0.6288546  0.99632119]\n",
      "   [0.80094484 0.96251272]]\n",
      "\n",
      "  [[0.67012954 0.80356619]\n",
      "   [0.91517917 0.83174796]]]\n",
      "\n",
      "\n",
      " [[[0.80458243 0.97712759]\n",
      "   [0.91272943 0.86171778]]\n",
      "\n",
      "  [[0.8827965  0.95316097]\n",
      "   [0.95877647 0.98136021]]\n",
      "\n",
      "  [[0.96840121 0.87088313]\n",
      "   [0.70449495 0.89625081]]]]\n",
      "Error : 0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1975)\n",
    "x=np.random.rand(2,3,23,23)\n",
    "\n",
    "hyper_param={\"f\":2, \"s\" :11}\n",
    "c=pool_forward(x,**hyper_param)\n",
    "\n",
    "pool_tf = tf.nn.max_pool(tf.transpose(x,(0,2,3,1)),[1,2,2,1],[1,11,11,1],'VALID') ## tensorflow api\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    pool_tf =sess.run(pool_tf)\n",
    "    \n",
    "pool_tf=np.transpose(pool_tf, (0,3,1,2))\n",
    "\n",
    "assert c.shape==pool_tf.shape\n",
    "assert (c==pool_tf).all()\n",
    "\n",
    "print(\"your implementation is correct\")\n",
    "print(\"output shape :\", c.shape)\n",
    "print(\"output :\", c)\n",
    "print(\"Error :\" ,np.sum((c-pool_tf)**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hr0tWI2qnPOp"
   },
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Error**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.0\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "            (2, 3, 2, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRCZdRWinPOr"
   },
   "source": [
    "### 4.2 - backward max pooling (3 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FN1kDBVhnPOt"
   },
   "source": [
    "**Exercise**  :implement the backward pass for the pooling layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpSvOKFWnPOy"
   },
   "outputs": [],
   "source": [
    "def pool_back(grad_pool, X, f, s):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "       - grad_pool : gradient of cost with respect to the output of the pooling layer\n",
    "       - X : input to pooling layer , numpy array with shape (N, C, IH, IW)\n",
    "       - f : int, filter size in height and width dim\n",
    "       - s :  int\n",
    "    Returns:\n",
    "       - dX_pool  : gradient of cost with respect to the input of the pooling layer, same shape as X\n",
    "    \"\"\"\n",
    "    dX_pool = None\n",
    "    ###########################################################################\n",
    "    # Implement the max pooling backward pass.                               #\n",
    "    ###########################################################################\n",
    "    \n",
    "    dX_pool=np.zeros_like(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return dX_pool\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uBU0WeUnPO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of dX : 0.12430622625821201\n",
      "dX[1,2,2:5,2:5] =  [[0.         0.         0.        ]\n",
      " [0.27722901 0.         0.78890722]\n",
      " [0.         0.68660425 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19)\n",
    "X=np.random.rand(2,3,10,10)\n",
    "g=np.random.rand(2,3,9,9)\n",
    "\n",
    "f=2\n",
    "s=1\n",
    "dX=pool_back(g, X, f, s)\n",
    "\n",
    "\n",
    "print(\"mean of dX :\",np.mean(dX))\n",
    "print(\"dX[1,2,2:5,2:5] = \",dX[1,2,2:5,2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qOjFOKonPO-"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dX =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.2428232587752177\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dX[1,2,2:5,2:5] =** \n",
    "</td>\n",
    "<td>\n",
    "[[0.         0.         1.09075724]\n",
    " [1.29276074 0.         1.15881724]\n",
    " [0.         0.61727634 0.        ]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbjFV9EwnPPA"
   },
   "source": [
    "### 5. Batch Normalization in CNN (Optional)(bonus grade)(6 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XewKYMkHnPPC"
   },
   "source": [
    "### 5.1 - BN forward (3 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2AYHTmcnPPD"
   },
   "source": [
    "A family of feature normalization methods, including BN(Batch Normalization), LN(Layer Normalization), perform the following computation:\n",
    "$$\\hat{x_i} = \\frac{1}{\\sigma_i} (x_i - \\mu_i)$$\n",
    "\n",
    "Here $x$ is the feature computed by a layer, and $i$ is an index.In the case of 2D images, $i = (i_N , i_C , i_H , i_W )$ is a 4D vector indexing the features in $(N, C, H, W )$ order, where $N$ is the batch axis, $C$ is the channel axis, and $H$ and $W$ are the spatial height and width axes.\n",
    "\n",
    "$μ$ and $\\sigma$ in  are the mean and standard deviation (std) computed by :\n",
    "\n",
    "$$μ_i = \\frac{1}{m}\\sum_{k \\in S_i} x_i $$\n",
    "\n",
    "$$\\sigma_i = \\sqrt{\\frac{1}{m}\\sum_{k \\in S_i}(x_k - μ_i)^2 + \\epsilon}$$\n",
    "\n",
    "$S_i$ is the set of pixels in which\n",
    "the mean and std are computed, and m is the size of this set.\n",
    "In **Batch Norm** the set $S_i$  is defined as : \n",
    "$$S_i = \\left\\{k | k_C = i_C \\right\\}$$\n",
    "\n",
    "the pixels sharing the same channel index are normalized together, i.e., for each channel, BN computes $\\mu$ and $\\sigma$ along the $(N, H, W )$ axes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuB8ippfnPPF"
   },
   "outputs": [],
   "source": [
    "def batch_norm_forward(X, gamma, beta):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    \n",
    "    Returns:\n",
    "    - BN: Output data, of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    BN =None\n",
    "    ###########################################################################\n",
    "    # Implement the forward pass for spatial batch normalization              #\n",
    "    ###########################################################################\n",
    "    n,c,h,w = X.shape\n",
    "    means = np.mean(X, axis=(0,2,3))\n",
    "    stds = np.std(X, axis=(0,2,3))\n",
    "    means = np.tile(means.reshape(1,c,1,1),(n,1,h,w))\n",
    "    stds = np.tile(stds.reshape(1,c,1,1),(n,1,h,w))\n",
    "    norm_X = (X-means)/stds\n",
    "    \n",
    "    gamma = np.tile(gamma.reshape(1,c,1,1),(n,1,h,w))\n",
    "    beta = np.tile(beta.reshape(1,c,1,1),(n,1,h,w))\n",
    "    \n",
    "    \n",
    "    BN = norm_X*gamma + beta\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return BN\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-_GOlpj1nPPK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [9.33463814 8.90909116 9.11056338]\n",
      "  Stds:  [3.61447857 3.19347686 3.5168142 ]\n",
      "After spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [-3.33066907e-16  4.44089210e-17 -1.11022302e-16]\n",
      "  Stds:  [1. 1. 1.]\n",
      "After spatial batch normalization (nontrivial gamma, beta):\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [6. 7. 8.]\n",
      "  Stds:  [3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "X = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print('Before spatial batch normalization:')\n",
    "print('  Shape: ', X.shape)\n",
    "print('  Means: ', X.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', X.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "out= batch_norm_forward(X, gamma, beta)\n",
    "print('After spatial batch normalization:')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', out.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "out= batch_norm_forward(X, gamma, beta)\n",
    "print('After spatial batch normalization (nontrivial gamma, beta):')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', out.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8VfnIKGnPPQ"
   },
   "source": [
    "### 5.2 - BN Backward (3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r_IylqknPPS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 5, 6)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_norm_backward(dout, X, gamma, beta):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout :\n",
    "    - X: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    \n",
    "    Returns:\n",
    "    - dX: Output data, of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    dX =None\n",
    "    ###########################################################################\n",
    "    # Implement the backward pass for spatial batch normalization             #\n",
    "    ###########################################################################\n",
    "    \n",
    "    n,c,h,w = X.shape\n",
    "    means = np.mean(X, axis=(0,2,3))\n",
    "    stds = np.std(X, axis=(0,2,3))\n",
    "    means = np.tile(means.reshape(1,c,1,1),(n,1,h,w))\n",
    "    stds = np.tile(stds.reshape(1,c,1,1),(n,1,h,w))\n",
    "    norm_X = (X-means)/stds\n",
    "    \n",
    "    gamma = np.tile(gamma.reshape(1,c,1,1),(n,1,h,w))\n",
    "    beta = np.tile(beta.reshape(1,c,1,1),(n,1,h,w))\n",
    "    \n",
    "    \n",
    "    BN = norm_X*gamma + beta\n",
    "    \n",
    "    dX = (1/n*stds)* (n*dout - np.sum(dout, axis=0) - BN*np.sum(dout*BN, axis=0))\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return dX\n",
    "    \n",
    "dout = np.random.randn(2,3,5,6)\n",
    "X = np.random.randn(2,3,5,6)\n",
    "gamma = np.random.randn(3)\n",
    "beta = np.random.randn(3)\n",
    "batch_norm_backward(dout, X, gamma, beta).shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_modify.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
